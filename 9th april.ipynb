{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f48f3d1-e0cc-47ef-9d95-10ca010a5571",
   "metadata": {},
   "source": [
    "# Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d3f899-9c17-48b2-bb81-eec154ae1e09",
   "metadata": {},
   "source": [
    "## Bayes' theorem is a fundamental principle in probability theory that describes how to update or revise the probability of a hypothesis or event based on new evidence or information. It is named after the Reverend Thomas Bayes, an 18th-century British statistician.\n",
    "## In its simplest form, Bayes' theorem states that the probability of a hypothesis (H) given some observed evidence (E) is proportional to the probability of the evidence given the hypothesis, multiplied by the prior probability of the hypothesis:\n",
    "## P(H|E) = P(E|H) * P(H) / P(E) , where: P(H|E) is the posterior probability of the hypothesis given the evidence, P(E|H) is the likelihood of the evidence given the hypothesis, P(H) is the prior probability of the hypothesis, P(E) is the probability of the evidence\n",
    "## Bayes' theorem is widely used in many fields, including statistics, machine learning, and artificial intelligence, to make predictions and make decisions based on uncertain information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1573bd0-6756-401c-aea4-480b7fdf78c2",
   "metadata": {},
   "source": [
    "# Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b62fd2-197c-464e-a066-3c41bcfc5026",
   "metadata": {},
   "source": [
    "## The formula for Bayes' theorem is: P(H|E) = P(E|H) * P(H) / P(E), where: P(H|E) is the posterior probability of the hypothesis H given the observed evidence E, P(E|H) is the likelihood of observing evidence E given the hypothesis H, P(H) is the prior probability of the hypothesis H before observing the evidence E, P(E) is the probability of observing the evidence E, which can be calculated as the sum of the likelihood of observing E given all possible hypotheses H:\n",
    "## P(E) = P(E|H1) * P(H1) + P(E|H2) * P(H2) + ... + P(E|Hn) * P(Hn), where H1, H2, ..., Hn are all possible hypotheses.\n",
    "## Bayes' theorem allows us to update the prior probability of a hypothesis H given new evidence E by calculating the posterior probability of H, which takes into account both the prior probability and the likelihood of the evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3788db-203f-42fa-aaf9-660f95477746",
   "metadata": {},
   "source": [
    "# Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeff566-8fe4-4d51-b1f7-14a899f49946",
   "metadata": {},
   "source": [
    "## Bayes' theorem is used in practice in a wide range of fields, including statistics, machine learning, artificial intelligence, and scientific research. Here are some examples of how Bayes' theorem is used in practice:\n",
    "## 1. Medical diagnosis: Bayes' theorem is used to update the probability of a disease given a positive or negative test result. The prior probability of the disease is updated based on the likelihood of the test result given the disease and the prevalence of the disease in the population.\n",
    "## 2. Spam filtering: Bayes' theorem is used to classify emails as spam or non-spam based on the probability of certain words or phrases appearing in the email. The prior probability is based on the frequency of these words in a training set of spam and non-spam emails, and the likelihood is calculated based on the probability of these words given the classification.\n",
    "## 3. Predictive modeling: Bayes' theorem is used in predictive modeling to update the probability of a hypothesis based on new data or evidence. For example, in machine learning, Bayes' theorem can be used to update the probability of a classification given new features of a data point.\n",
    "## 4. Decision-making: Bayes' theorem can be used to make decisions based on uncertain information. For example, in financial decision-making, Bayes' theorem can be used to update the probability of a stock price increase or decrease based on new information such as company earnings reports.\n",
    "## Overall, Bayes' theorem provides a powerful framework for reasoning under uncertainty and updating beliefs based on new evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11193c2-267d-456c-9c32-9f66ff3dd6d2",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3b0d55-b950-4cee-a87e-8a0cc29bf086",
   "metadata": {},
   "source": [
    "## Bayes' theorem is a relationship between conditional probabilities. In fact, Bayes' theorem can be derived from the definition of conditional probability.\n",
    "## Conditional probability is the probability of an event A given that another event B has occurred, and it is denoted as P(A|B). Bayes' theorem relates this conditional probability to the inverse conditional probability, P(B|A).\n",
    "## Bayes' theorem states that: P(A|B) = P(B|A) * P(A) / P(B), where: P(A|B) is the conditional probability of event A given that event B has occurred, P(B|A) is the conditional probability of event B given that event A has occurred, P(A) is the prior probability of event A, P(B) is the prior probability of event B\n",
    "## Bayes' theorem provides a way to update the prior probability of an event A given new information about event B. By multiplying the prior probability of A by the likelihood of observing B given A, and dividing by the overall probability of observing B, we obtain the posterior probability of A given B.\n",
    "## In summary, Bayes' theorem is a formula for calculating the conditional probability of an event given new evidence, and it can be derived from the definition of conditional probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4313dcf8-feff-4d8f-a4cc-43275f287e36",
   "metadata": {},
   "source": [
    "# Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d011a02-594e-44f0-8a4f-8055588f045f",
   "metadata": {},
   "source": [
    "## Naive Bayes is a family of probabilistic classifiers that use Bayes' theorem to predict the probability of a class given a set of input features. The main types of Naive Bayes classifiers are Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. Here are some guidelines for choosing which type of Naive Bayes classifier to use for a given problem:\n",
    "## 1. Gaussian Naive Bayes: This classifier assumes that the input features are normally distributed. Therefore, it is suitable for continuous input variables that follow a Gaussian distribution. Gaussian Naive Bayes can be used for problems such as predicting a numeric value or classifying data with continuous variables.\n",
    "## 2. Multinomial Naive Bayes: This classifier is suitable for discrete input variables that represent counts or frequencies, such as word counts in text classification. It is commonly used for text classification, spam filtering, and sentiment analysis.\n",
    "## 3. Bernoulli Naive Bayes: This classifier is a special case of the Multinomial Naive Bayes classifier that assumes that the input variables are binary, i.e., they can take on only two values, such as 0 or 1. It is often used for problems such as text classification or image classification, where the presence or absence of certain features is important.\n",
    "## In general, the choice of which type of Naive Bayes classifier to use depends on the nature of the input variables and the problem at hand. It is recommended to try all three types of classifiers and compare their performance on the problem dataset. It is also important to consider the assumptions made by each type of classifier and whether they are appropriate for the problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239e1ea-5707-489c-9bbb-c3b4d0501d47",
   "metadata": {},
   "source": [
    "# Q6. Assignment: You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:\n",
    "# Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "# A      3    3     4    4    3   3    3\n",
    "# B      2    2     1    2    2   2    3\n",
    "# Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad8142-cff5-4a29-b0f2-ea9569cfadca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To use Naive Bayes to classify a new instance with features X1=3 and X2=4, we need to calculate the posterior probabilities of each class given these feature values using Bayes' theorem.\n",
    "## Let's start by calculating the prior probabilities of each class, assuming equal prior probabilities: P(A) = 0.5, P(B) = 0.5\n",
    "## Now, we need to calculate the likelihood of each feature value given each class. Since we are assuming a Naive Bayes model, we can calculate the likelihood of each feature independently of the other features.\n",
    "## For feature X1=3, the likelihood of class A is: P(X1=3|A) = 4/10 = 0.4\n",
    "## The likelihood of class B is: P(X1=3|B) = 1/7 ≈ 0.143\n",
    "## For feature X2=4, the likelihood of class A is: P(X2=4|A) = 3/13 = 0.23\n",
    "## The likelihood of class B is: P(X2=4|B) = 3/9 ≈ 0.33\n",
    "## Now, we can use Bayes' theorem to calculate the posterior probability of each class given the feature values X1=3 and X2=4:\n",
    "## P(A|X1=3,X2=4) = P(X1=3|A) * P(X2=4|A) * P(A) / P(X1=3,X2=4)\n",
    "## P(B|X1=3,X2=4) = P(X1=3|B) * P(X2=4|B) * P(B) / P(X1=3,X2=4)\n",
    "## To calculate the denominator P(X1=3,X2=4), we can use the law of total probability: P(X1=3,X2=4) = P(X1=3|A) * P(X2=4|A) * P(A) + P(X1=3|B) * P(X2=4|B) * P(B)\n",
    "## Plugging in the values, we get:\n",
    "## P(X1=3,X2=4) = 0.4 * 0.23 * 0.5 + 0.143 * 0.33 * 0.5 ≈ 0.0695\n",
    "## Now we can calculate the posterior probabilities of each class:\n",
    "## P(A|X1=3,X2=4) = 0.4 * 0.23 * 0.5 / 0.0695 ≈ 0.\n",
    "## P(B|X1=3,X2=4) = 0.143 * 0.33 * 0.5 / 0.034 ≈ 0.118\n",
    "\n",
    "Therefore, Naive Bayes would predict that the new instance with features X1=3 and X2=4 belongs to class A, since it has a higher posterior probability than class B."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
